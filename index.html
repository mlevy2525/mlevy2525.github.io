<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Mara Levy</title>

    <meta name="author" content="Mara Levy">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Mara Levy
                </p>
                <p> I am a PhD student in the <a href="https://www.cs.umd.edu/"> Computer Science </a> department at the University of Maryland, College Park. I work on robotic learning and computer vision with <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>. I graduated Summa Cum Laude from the <a href="https://www.upenn.edu/">University of Pennsylvania</a> where I majored in Computer Science and minored in Mathematics.
                </p>
                <p style="text-align:center">
                  <a href="mailto:mlevy@umd.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Mara_Resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=qPQVUf4AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mlevy2525">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research utilizes a mixture of computer vision and robotics approaches to try and make robots more useful in the real world. Currently I am interested in how robots can learn from a very small number of demonstrations. In the long term I am interested in using these techniques to learn from online videos of humans. These learned skills can then be combined for long term planning.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
      <div class="two" id='smerf_image'>
        <img src='images/ardup_teaser.jpg' width=100%>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a>
        <span class="papertitle">	ARDuP: Active Region Video Diffusion for Universal Policies</span>
      </a>
      <br>
    <a href="https://scholar.google.com/citations?user=j-P28bQAAAAJ&hl=en">Shuaiyi Huang</a>,
    <strong>Mara Levy</strong>,
    <a href="https://zhenyujiang.me/">Zhenyu Jiang</a>,
    <a href="https://www.eas.caltech.edu/people/anima">Anima Anandkumar</a>,
    <a href="https://yukezhu.me/">Yuke Zhua</a>,
    <a href="https://jimfan.me/">Linxi Fan</a>,
    <a href="https://ai.stanford.edu/~dahuang/">De-An Huang</a>,
    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
        <br>
        <em>Under Submission</em>
        <br>
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a> -->
        <p></p>
        <p>
          A novel method for learning complex goal-conditioned robotics tasks from a single demonstration, using unique reward function and knowledge expansion.
        </p>
        <!-- <a href="https://waypoint-ex.github.io/">Project</a>
        /
        <a href="https://waypoint-ex.github.io/">Paper</a> -->
    </td>
    </tr>

    <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
      <div class="two" id='smerf_image'>
        <img src='images/VVIPE_Teaser.png' width=100%>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a>
        <span class="papertitle">	V-VIPE: Variational View Invariant Pose Embedding</span>
      </a>
      <br>
    <strong>Mara Levy</strong>,
		<a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
        <br>
        <em>Conference on Computer Vision and Pattern Recognition (CVPR) RHOBIN Workshop</em>, 2024
        <br>
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a> -->
        <p></p>
        <p>
          A method for projecting poses into an embedding space that is view invariant. This means the same pose shot from different camera angles should have an equal embedding.
        </p>
        <!-- <a href="https://waypoint-ex.github.io/">Project</a>
        / -->
        <a href="data/VVIPE_Camera_Ready.pdf.pdf">Paper</a>
    </td>
    </tr>

    <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
      <div class="two" id='smerf_image'>
        <img src='images/waypoint_teaser.png' width=100%>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://waypoint-ex.github.io/">
        <span class="papertitle">	WayEx: Waypoint Exploration using a Single demonstration</span>
      </a>
      <br>
    <strong>Mara Levy</strong>,
		<a href="https://www.cs.umd.edu/~nirat/">Nirat Saini</a>,
		<a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
        <br>
        <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024
        <br>
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a> -->
        <p></p>
        <p>
          A novel method for learning complex goal-conditioned robotics tasks from a single demonstration, using unique reward function and knowledge expansion.
        </p>
        <a href="https://waypoint-ex.github.io/">Project</a>
        /
        <a href="data/wayex_paper.pdf">Paper</a>
    </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
        <div class="two" id='smerf_image'>
          <img src='images/coarse_metro_teaser.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="data/CoarseMETRO_cameraready.pdf">
          <span class="papertitle">Coarse-to-Fine Human Mesh Recovery with Transformers</span>
        </a>
        <br>
      <a href="https://vatsalag99.github.io/">Vatsal Agarwal</a>,
      <strong>Mara Levy</strong>,
      <a href="https://maxlikelihood.ai/">Max Ehrlich</a>,
      <a href="https://tangyoubao.github.io/">Youbao Tang</a>,
      <a href="https://scholar.google.com/citations?user=hLCSti0AAAAJ&hl=en">Ning Zhang</a>,
      <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
          <br>
          <em>Under Submission</em>
          <br>
          <p></p>
          <p>
            Build efficient Transformer design for non-parametric human mesh recovery with coarse-to-fine pipeline.
          </p>
          <!-- <a href="https://waypoint-ex.github.io/">Project</a> -->
          <a href="data/CoarseMETRO_cameraready.pdf">Paper</a>
      </td>
      </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
        <div class="two" id='smerf_image'>
          <img src='images/no-frills_teaser.gif' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://mlevy2525.github.io/DynamicAddOn/">
          <span class="papertitle">	No-frills Dynamic Planning using Static Planners</span>
        </a>
        <br>
      <strong>Mara Levy</strong>,
      <a> Vasista Ayyagari</a>,
      <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
          <br>
          <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2021
          <br>
          <!-- / -->
          <!-- <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
          /
          <a href="https://arxiv.org/abs/2312.07541">arXiv</a> -->
          <p></p>
          <p>
            A planning algorithm that helps reinforcement learning algorithms that can typically only solve tasks for static objects solve the same task, but with a dynamic object.
          </p>
          <a href="https://mlevy2525.github.io/DynamicAddOn/">Project</a>
          /
          <a href="https://arxiv.org/abs/2106.09714">Paper</a>
      </td>
      </tr>

    
    


            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template Credits <a href="https://jonbarron.info/"> Jon Barron </a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
